# Extensible corpus mix for continual pretraining.
# You can add/remove items without touching Python code.
sources:
  # Khmer high-quality seed corpus
  - name: khmer_hq_seed
    language: khmer
    dataset: kimleang123/khmer-text-dataset
    split: train
    text_column: text
    weight: 0.35

  # Khmer web-scale source (Common Crawl derived)
  - name: khmer_mc4
    language: khmer
    dataset: allenai/c4
    subset: km
    split: train
    text_column: text
    streaming: true
    max_samples: 300000
    weight: 0.15

  # Khmer Wikipedia dump (if available in your chosen dataset namespace)
  # Replace with a valid HF dataset id/subset that provides Khmer wiki text.
  - name: khmer_wikipedia
    language: khmer
    dataset: wikimedia/wikipedia
    subset: '20231101.km'
    split: train
    text_column: text
    max_samples: 200000
    weight: 0.10

  # English high-quality sentence corpus
  - name: english_hq_seed
    language: english
    dataset: agentlans/high-quality-english-sentences
    split: train
    text_column: text
    weight: 0.20

  # English web-scale source (sampled with streaming to avoid full C4 download)
  - name: english_c4_sampled
    language: english
    dataset: allenai/c4
    subset: en
    split: train
    text_column: text
    streaming: true
    max_samples: 500000
    weight: 0.20

  # Alternative smaller English option (recommended on constrained environments):
  # - name: english_fineweb_edu_small
  #   language: english
  #   dataset: HuggingFaceFW/fineweb-edu
  #   split: train
  #   text_column: text
  #   max_samples: 500000
  #   weight: 0.20
