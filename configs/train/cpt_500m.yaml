seed: 42
dataset_dir: data/mixture
tokenizer_dir: artifacts/tokenizer
model_config: configs/model/gpt2_500m.json
deepspeed: configs/deepspeed/zero2_h100.json
training:
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 16
  learning_rate: 3e-4
  num_train_epochs: 1.0
  max_length: 2048
  warmup_ratio: 0.03
