seed: 42
model: google/gemma-3-1b-it
dataset_dir: data/mixture_train
dataset_split: train
text_column: text
deepspeed: configs/deepspeed/zero2_h100.json
training:
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 16
  learning_rate: 2e-5
  adam_beta1: 0.9
  adam_beta2: 0.95
  weight_decay: 0.1
  warmup_steps: 200
  num_train_epochs: 1.0
  max_length: 2048
  gradient_checkpointing: true
  attn_implementation: sdpa
  save_strategy: steps
  save_steps: 1000
  save_total_limit: 5
